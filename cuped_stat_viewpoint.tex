\documentclass[10pt, a4paper]{article}

\usepackage{etex} % расширение классического tex
% в частности позволяет подгружать гораздо больше пакетов, чем мы и займёмся далее

%\usepackage{mathtext} % русские буквы в формулах? (и без неё работает)
% Например, $x_{\text{один}}$


\usepackage{verbatim} % для многострочных комментариев
\usepackage{makeidx} % для создания предметных указателей



\usepackage{setspace}
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{mathrsfs} % sudo yum install texlive-rsfs
\usepackage{dsfont} % sudo yum install texlive-doublestroke
\usepackage{array, multicol, multirow, bigstrut} % sudo yum install texlive-multirow
\usepackage{indentfirst} % установка отступа в первом абзаце главы

\usepackage{physics} % много вкусного


\usepackage{bm}
\usepackage{bbm} % шрифт с двойными буквами
%\usepackage[perpage]{footmisc}

\usepackage{dcolumn} % центрирование по разделителю для apsrtable

% создание гиперссылок в pdf
\usepackage[unicode, colorlinks=true, urlcolor=blue, hyperindex, breaklinks]{hyperref}


\usepackage{microtype} % свешиваем пунктуацию
% теперь знаки пунктуации могут вылезать за правую границу текста, при этом текст выглядит ровнее


\usepackage{textcomp}  % Чтобы в формулах можно было русские буквы писать через \text{}

% размер листа бумаги
%\usepackage[paperwidth=145mm,paperheight=215mm,
%height=182mm,width=113mm,top=20mm,includefoot]%{geometry}
\usepackage[paper=a4paper, top=15mm, bottom=13.5mm, left=16.5mm, right=13.5mm, includefoot]{geometry}

\usepackage{xcolor}

% \usepackage[pdftex]{graphicx} % для вставки графики, убрано, т.к. knitr похоже сам добавляет

\usepackage{float, longtable}
\usepackage{soulutf8}

\usepackage{enumitem} % дополнительные плюшки для списков
%  например \begin{enumerate}[resume] позволяет продолжить нумерацию в новом списке

\usepackage{mathtools}
\usepackage{cancel, xspace} % sudo yum install texlive-cancel

% \usepackage{minted} % display program code with syntax highlighting
% требует установки pygments и python

\usepackage{numprint} % sudo yum install texlive-numprint
\npthousandsep{,}\npthousandthpartsep{}\npdecimalsign{.}


\usepackage{subfigure} % для создания нескольких рисунков внутри одного

\usepackage{tikz, pgfplots} % язык для рисования графики из latex'a
\pgfplotsset{compat=1.16}
\usetikzlibrary{trees} % tikz-прибамбас для рисовки деревьев
\usepackage{tikz-qtree} % альтернативный tikz-прибамбас для рисовки деревьев
\usetikzlibrary{arrows} % tikz-прибамбас для рисовки стрелочек подлиннее

\usepackage{todonotes} % для вставки в документ заметок о том, что осталось сделать
% \todo{Здесь надо коэффициенты исправить}
% \missingfigure{Здесь будет Последний день Помпеи}
% \listoftodos --- печатает все поставленные \todo'шки



\usepackage{booktabs} %  красивые таблицы
% заповеди из докупентации:
% 1. Не используйте вертикальные линни
% 2. Не используйте двойные линии
% 3. Единицы измерения - в шапку таблицы
% 4. Не сокращайте .1 вместо 0.1
% 5. Повторяющееся значение повторяйте, а не говорите "то же"


\usepackage{fontspec} % что-то про шрифты?
\usepackage{polyglossia} % русификация xelatex
\usepackage{csquotes}

\setmainlanguage{english}
\setotherlanguages{english}

% download "Linux Libertine" fonts:
% http://www.linuxlibertine.org/index.php?id=91&L=1
\setmainfont{Linux Libertine O} % or Helvetica, Arial, Cambria
% why do we need \newfontfamily:
% http://tex.stackexchange.com/questions/91507/
\newfontfamily{\cyrillicfonttt}{Linux Libertine O}

\AddEnumerateCounter{\asbuk}{\russian@alph}{щ} % для списков с русскими буквами
\setlist[enumerate, 1]{label=\asbuk*),ref=\asbuk*}





% Операторы
\DeclareMathOperator*\plim{plim}
\DeclareMathOperator{\sgn}{sign}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\amn}{arg\,min}
\DeclareMathOperator*{\amx}{arg\,max}
\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\sVar}{sVar}
\DeclareMathOperator{\hVar}{\widehat{\Var}}

\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\sCov}{sCov}
\DeclareMathOperator{\Corr}{Corr}
\DeclareMathOperator{\pCorr}{pCorr}
\DeclareMathOperator{\E}{\mathbb{E}}
\let\P\relax
\DeclareMathOperator{\P}{\mathbb{P}}
\renewcommand{\le}{\leqslant}
\renewcommand{\ge}{\geqslant}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}

% Распределения
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cBinom}{\mathcal{Binom}}
\newcommand{\cPois}{\mathcal{Pois}}
\newcommand{\cBeta}{\mathcal{Beta}}
\newcommand{\cGamma}{\mathcal{Gamma}}

% Множества
\def \R{\mathbb{R}}
\def \N{\mathbb{N}}
\def \Z{\mathbb{Z}}

% Другое
\newcommand{\dx}[1]{\,\mathrm{d}#1} % Для интеграла: маленький отступ и прямая d
\newcommand{\ind}[1]{\mathbbm{1}_{\{#1\}}} % Индикатор события
\newcommand{\iid}{\mathrel{\stackrel{\rm i.\,i.\,d.}\sim}}
\newcommand{\const}{\mathrm{const}}


\newtheorem{theorem}{Theorem}

\title{CUPED: statistician viewpoint}
\author{Boris Demeshev}

\begin{document}

\begin{abstract}
    I try to give honest statistical background to the CUPED method.
    This allows us to correctly include multiple predictors and 
    use heteroscedasticity robust standard errors.
\end{abstract}


\maketitle

\section{Déjà vu}

On the third page Deng writes ‘the linear model makes strong assumptions that
are usually not satisfied in practice, i.e., the conditional expectation of the outcome metric is linear in the treatment
assignment and covariates. In addition, it also requires all
residuals to have a common variance’.

As I am teaching statistics and econometrics I was eager to read further. 
But than I encounter $\theta = \Cov(Y, X) / \Var(X)$ in equation 4 which is a theoretical
counterpart of slope estimate in simple regression. 
And later t-test is applied to $\Delta_{cv}$ that is again equivalent to a second simple regression.
Regression is replaced by something similar to two regressions. Déjà vu.

So I decided to expose the CUPED method using old boring regression language. 
Let's see what will hapen!

\section{Old regression friend}

To simplify the use of regression language I will start with one dataset of $n$ observations
with three variables:

\begin{itemize}
    \item $w_i$ the indicator of treatment: $w_i = 1$ for the treated group and $w_i=0$ for 
    the untreated group.
    \item $x_i$ any covariate that is a-priori independent with treatment indicator $w_i$.
    \item $y_i$ the target variable that is probably dependent both with $w_i$ and 
    $x_i$. 
\end{itemize}

Using regression language CUPED is a two step procedure:

\textbf{Step 1a}. Estimate the following regression using OLS: 
\[
    \hat y_i = \hat \gamma_1 + \hat\gamma_2 w_i + \hat \theta x_i.
\]

\textbf{Step 1b}. Calculate the semiresidual $r_i = y_i - \hat \theta x_i$.

I call this $r_i$ 'semiresudual' as classic residual in econometrics is 
\[
\hat u_i = y_i - \hat y_i = y_i - (\hat\gamma_1 + \hat\gamma_2 w_i + \hat\theta x_i).
\]


\textbf{Step 2a}. Estimate the second regression using OLS:
\[
    \hat r_i = \hat \beta_1 + \hat \beta_2 w_i.
\]

\textbf{Step 2b}. Use classical standard errors to build confidence interval for $\beta_2$.


Why this two-step procedure is better than just the multivariate regression
of the first step with confidence interval for $\gamma_2$?


Honestly speaking Deng is not very explicit which regression should be used in the first step.  
On the page three the theoretical unknown $\theta$ is used. 

So one may also consider a simplier alternative regression $\hat y_i = \hat \gamma_1 + \hat \theta x_i$.
I will discuss why I prefer the inclusion of $w_i$ as regressor in the first step.


\section{Comparison with multivariate regression}

Let's talk about numeric estimates withoud assumptions at all. 

\begin{theorem}
    The proposed two step procedure gives exaclty the same estimates for both steps: $\hat\beta_1 = \hat\gamma_1$,
    $\hat\beta_2 = \hat\gamma_2$, both residual vectors are also equal, $y_i - \hat y_i = r_i - \hat r_i$.

    The residual sums of squares are equal for both steps
    \[
     RSS = \sum (y_i - \hat y_i)^2 = \sum (r_i - \hat r_i)^2.
    \]
\end{theorem}

\begin{proof}
Just remark that multivariate regression minimizes $\sum (y_i - \hat y_i)^2$ with 
respect to the three parameters $\hat\gamma_1$, $\hat\gamma_2$, $\hat\theta$ simulteneously.

Two step procedure has the same optimal $\hat \theta$ by design and on step two minimizes 
the same sum with respect to $\hat\beta_1$ and $\hat beta_2$ 
with $\hat\gamma$ fixed at the optimal value.
\end{proof}

Hence all the difference between the two approaches lies in standard errors and confidence intervals.
To discuss the validity of standard errors we'll need some assumptions.


Let's start easy first. No heteroscedasticity and no interaction between treatment $w_i$ and covariate $x_i$.
Correctly specified linear model. 

Assume that the true model is 
\[
y_i = \gamma_1 + \gamma_2 w_i + \theta x_i + u_i.
\]

The observations are independent and identically distributed with finite forth moments. 
The error term $u_i$ satisfies $\E(u_i \mid X) = 0$, $\Var(u_i \mid X) = \sigma^2$.

[here goes the picture]


It is well known in econometrics that OLS estimator $\hat\gamma_2$ is unbiased and consistent in this case.
As $\hat\beta_2$ estimate from second step is exactly equal to $\hat\gamma_2$ the same result applies.

And what about standard errors?

Here I present a small summary of classic result in econometrics. 
I find this useful for the completeness of the text. 

Let's introduce the matrix of all predictors and its centered version.
\[
X = \begin{pmatrix}
    1 & w_1 & x_1 \\
    1 & w_2 & x_2 \\
    \vdots & \vdots & \vdots \\
    1 & w_n & x_n \\
\end{pmatrix},
X_c = \begin{pmatrix}
    w_1 - \bar w & x_1 - \bar x \\
    w_2 - \bar w & x_2 - \bar x\\
    \vdots & \vdots & \vdots \\
    w_n - \bar w& x_n  - \bar x \\
\end{pmatrix}.    
\]


Under our assumptions the true variance of estimates is
\[
\Var\left( 
\begin{pmatrix}    
    \hat \gamma_1 \\
    \hat \gamma_2 \\
    \hat \theta
\end{pmatrix}
\mid X \right) = \sigma^2 (X^TX)^{-1}.
\]

The default estimator of unknown $\sigma^2$ in multivariate regression with $k$ 
estimated coefficients is given by 
\[
\hat\sigma^2 = \frac{RSS}{n - k} (X^TX)^{-1}.
\]

Hence in our case the default estimate of the variance of $\hat\beta_2$ will be found 
in the matrix
\[
        \hVar\left( 
        \begin{pmatrix}    
            \hat \gamma_1 \\
            \hat \gamma_2 \\
            \hat \theta
        \end{pmatrix}
        \mid X \right) = \frac{RSS}{n - 3} (X^TX)^{-1}.
\]



If we ignore the $\hat\gamma_1$ and focus just on $\hat\gamma_2$ and $\hat\theta$ then 
our regression is equivalent to regression in centered variables.

\[
\Var\left( 
    \begin{pmatrix}    
        \hat \gamma_2 \\
        \hat \theta
    \end{pmatrix}
    \mid X \right) = \sigma^2 (X_c^TX_c)^{-1}.
\]

This is the FWL theorem or this result may be proven by block-matrix inversion. 


Let's look closer on the conditional covariance matrix.
I rewrite it as 
\[
    \Var\left( 
        \begin{pmatrix}    
            \hat \gamma_2 \\
            \hat \theta
        \end{pmatrix}
        \mid X \right) = \frac{\sigma^2}{n-1} \left(\frac{X_c^TX_c}{n-1}\right)^{-1}.        
\]

And $\left(\frac{X_c^TX_c}{n-1}\right)$ is exactly the sample covariance matrix of the treatment 
indicator $w_i$ and covariate $x_i$, 
\[
    \left(\frac{X_c^TX_c}{n-1}\right) = \sVar\left(
        \begin{pmatrix}    
            w \\
            x \\
        \end{pmatrix}
    \right) =  
    \begin{pmatrix}    
        \sVar(w) & \sCov(w, x) \\
        \sCov(w, x) & \sVar(x) \\
    \end{pmatrix}
    =
    \begin{pmatrix}    
        \frac{\sum (w_i - \bar w)^2}{n-1} & \frac{\sum (w_i - \bar w)(x_i - \bar x)}{n-1} \\
        \frac{\sum (w_i - \bar w)(x_i - \bar x)}{n-1} & \frac{\sum (x_i - \bar x)^2}{n-1} \\
    \end{pmatrix}.    
\]


The sample covariance matrix with the growing number of observations will tend in probability to the true 
covariance matrix 
\[
\plim_{n\to\infty} \sVar\left(
    \begin{pmatrix}    
        w \\
        x \\
    \end{pmatrix}
\right) = 
\Var\left(
    \begin{pmatrix}    
        w_i \\
        x_i \\
    \end{pmatrix}
\right) =     \begin{pmatrix}    
    \Var(w_i) & \Cov(w_i, x_i) \\
    \Cov(w_i, x_i) & \Var(x_i) \\
\end{pmatrix}.
\]

What we know about the true covariance matrix?

The treatment is assumed to be independent of the covariate, hence $\Cov(w_i, x_i) = 0$.
If the probability of being treated is equal to $1/2$ then $\Var(w_i) = 1/4$.

The classical estimate of covariance matrix is given by 
\[
\hVar\left( 
    \begin{pmatrix}    
        \hat \gamma_2 \\
        \hat \theta
    \end{pmatrix}
    \mid X \right) = 
\frac{RSS/(n-3)}{n-1} \left(\frac{X_c^TX_c}{n-1}\right)^{-1}.
\]

Let's replaced this classic estimator of covariance matrix by an estimator that uses a-priori information 
about zero covariance $\Cov(w_i, x_i) = 0$:
\[
\hVar_{cuped}\left( 
    \begin{pmatrix}    
        \hat \gamma_2 \\
        \hat \theta
    \end{pmatrix}
    \mid X \right) = 
\frac{RSS/(n-3)}{n-1} 
\begin{pmatrix}
\sVar(w) & 0 \\
0 & \sVar(x) \\
\end{pmatrix}
^{-1}.
\]

The inversion of a diagonal matrix is easy and we get
\[
\hVar_{cuped}(\hat\gamma_2) = \frac{RSS/(n-3)}{n-1} \sVar(w)^{-1}.    
\]


How this compares with the CUPED two-step procedure?

The default variance estimate is equal to
\[
    \hVar(\hat\beta_2) = \frac{RSS}{n-2} \frac{1}{\sum (w_i - \bar w)^2} = \frac{RSS/(n-2)}{n-1} \sVar(w)^{-1}.
\]

And they match almost perfectly! Now we understand what CUPED two-step procudure does!

Ignoring the offset factor $(n-2)/(n-3)$ the CUPED replaces the sample 
covariance of $w$ and $x$ by zero in the construction of a confidence interval for the effect. 

It seems intuitive that using a-priori information is always better than ignoring it. 
But the reality may seem a little bit surprising. 

Imagine that Alice uses a-priori information about zero covariance of treatment and covariate. 
And Bob uses an old multivariate regression that ignores this fact. 
They both construct 95\% confidence intervals for the effect. 

We have proven the point estimates of Alice and Bob are exactly equal. 
As Alice uses more information her intervals should be shorter. 
And we are in a contradiction now. 
If Alice's interval is always shorter than Bob's interval then they cannot 
both cover unknown parameter with probability 95\%.

The use of a-priori information about zero covariance will sometime lead to a wider 
confidence interval for the effect. 

Let's study this counter-intuitive result with a toy problem. 

\section{Toy problem to really understand the difference}

Assume $y_i$ are independent and normally distributed $\cN(\mu, 1)$.
Alice knows the true variance $\sigma^2 = 1$ and Bob uses the estimate 
\[
\hat\sigma^2 = \sVar(y) = \frac{\sum(y_i - \bar y)^2}{n-1}.    
\]

They both build 95\% confidence intervals for $\mu$. 
Alice uses normal distribution and Bob uses t-distribution with $(n-1)$ degrees of freedom. 

For simplicity let's assume that $n=10$. 

Alice's confidence interval is 
\[
[\bar y - 1.96 \cdot 1/\sqrt{10}; \bar y - 1.96 \cdot 1/sqrt{10}].
\]

Bob's confidence interval is 
\[
[\bar y - 2.26 \cdot \hat\sigma/\sqrt{10}; \bar y - 2.26 \cdot \hat\sigma/sqrt{10}].        
\]

Here $1.96$ and $2.26$ are $0.975$ quantiles of 
standard normal and t distribution with 9 degrees of freedom. 


We immediately see that sometimes Bob's interval will be shorter and sometimes Alice's interval will. 

When the number of observations will tend to infinity the quantiles will coinced, 
$\plim \hat\sigma =1$, 
so the intervals will be exactly the same. 


\section{Heteroscedasticity case}


\section{Unanswered questions}




\end{document}